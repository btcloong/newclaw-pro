name: Auto Crawl RSS Feeds

on:
  schedule:
    # 每30分钟运行一次
    - cron: '*/30 * * * *'
    # 每天凌晨2点全量抓取
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      crawl_type:
        description: 'Crawl type'
        required: true
        default: 'auto'
        type: choice
        options:
          - auto
          - full
          - high_priority
          - process_ai

env:
  NODE_VERSION: '20'

jobs:
  crawl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Setup environment
      run: |
        echo "GEMINI_API_KEY=${{ secrets.GEMINI_API_KEY }}" > .env.local
        echo "API_KEY=${{ secrets.API_KEY }}" >> .env.local
    
    - name: Run crawler
      run: |
        CRAWL_TYPE="${{ github.event.inputs.crawl_type || 'auto' }}"
        
        # Map workflow inputs to CLI commands
        if [ "$CRAWL_TYPE" = "full" ]; then
          CLI_CMD="full"
        elif [ "$CRAWL_TYPE" = "high_priority" ]; then
          CLI_CMD="high"
        elif [ "$CRAWL_TYPE" = "process_ai" ]; then
          CLI_CMD="ai 10"
        else
          CLI_CMD="auto"
        fi
        
        echo "Running: npx tsx src/lib/crawler-cli.ts $CLI_CMD"
        npx tsx src/lib/crawler-cli.ts $CLI_CMD
    
    - name: Upload crawl results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: crawl-results
        path: |
          data/
          !data/**/*.log
        retention-days: 7
    
    - name: Sync to Vercel
      if: success()
      env:
        VERCEL_URL: ${{ secrets.VERCEL_URL }}
        API_KEY: ${{ secrets.API_KEY }}
      run: |
        echo "Syncing crawl results to Vercel..."
        curl -X POST "$VERCEL_URL/api/crawl" \
          -H "Content-Type: application/json" \
          -H "X-API-Key: $API_KEY" \
          -d '{"type":"auto","limit":10}' \
          --max-time 300 || echo "Vercel sync triggered"

    - name: Sync to Alibaba Cloud Server
      if: success()
      env:
        SERVER_HOST: ${{ secrets.SERVER_HOST }}
        SERVER_USER: ${{ secrets.SERVER_USER }}
        SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
      run: |
        echo "Syncing crawl results to Alibaba Cloud server..."
        
        # Setup SSH
        mkdir -p ~/.ssh
        echo "$SSH_PRIVATE_KEY" > ~/.ssh/deploy_key
        chmod 600 ~/.ssh/deploy_key
        ssh-keyscan -H $SERVER_HOST >> ~/.ssh/known_hosts 2>/dev/null
        
        # Sync data directory to server
        rsync -avz --delete \
          -e "ssh -i ~/.ssh/deploy_key -o StrictHostKeyChecking=no" \
          ./data/ $SERVER_USER@$SERVER_HOST:/home/admin/newclaw-pro/data/
        
        echo "Data synced successfully"
        
        # Trigger server to reload data (optional - restart pm2)
        ssh -i ~/.ssh/deploy_key -o StrictHostKeyChecking=no \
          $SERVER_USER@$SERVER_HOST \
          "cd /home/admin/newclaw-pro && pm2 reload newclaw-pro || echo 'PM2 reload skipped'"
    
    - name: Notify on failure
      if: failure()
      run: |
        echo "::error::Crawl failed! Check logs for details."
        exit 1
