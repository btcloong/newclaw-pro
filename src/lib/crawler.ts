import Parser from "rss-parser";
import { db, NewsItem, Project } from "./db";

const rssParser = new Parser({
  timeout: 10000,
  headers: {
    'User-Agent': 'Mozilla/5.0 (compatible; NewClawBot/1.0)'
  }
});

// çœŸå®çš„ RSS æ–°é—»æºé…ç½®
const RSS_SOURCES = {
  // AI å…¬å¸å®˜æ–¹åšå®¢
  openai: {
    url: "https://openai.com/blog/rss.xml",
    name: "OpenAI Blog",
    category: "å¤§æ¨¡å‹"
  },
  googleAI: {
    url: "https://blog.google/technology/ai/rss/",
    name: "Google AI Blog",
    category: "å¤§æ¨¡å‹"
  },
  anthropic: {
    url: "https://www.anthropic.com/news/rss.xml",
    name: "Anthropic",
    category: "å¤§æ¨¡å‹"
  },
  // ç§‘æŠ€åª’ä½“
  techcrunchAI: {
    url: "https://techcrunch.com/category/artificial-intelligence/feed/",
    name: "TechCrunch AI",
    category: "AIèµ„è®¯"
  },
  theVergeAI: {
    url: "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
    name: "The Verge AI",
    category: "AIèµ„è®¯"
  },
  // å­¦æœ¯èµ„æº
  arxivAI: {
    url: "http://export.arxiv.org/rss/cs.AI",
    name: "arXiv AI",
    category: "å­¦æœ¯ç ”ç©¶"
  },
};

// æŠ“å– RSS æ–°é—»
export async function crawlRSS(): Promise<{ success: boolean; count: number; errors: string[] }> {
  const errors: string[] = [];
  let totalItems = 0;

  for (const [key, source] of Object.entries(RSS_SOURCES)) {
    try {
      console.log(`[RSS] Crawling ${source.name}...`);
      
      const feed = await rssParser.parseURL(source.url);
      
      for (const item of feed.items.slice(0, 5)) {
        const id = `rss_${key}_${Date.now()}_${Math.random().toString(36).slice(2, 8)}`;
        
        const newsItem: NewsItem = {
          id,
          title: item.title?.slice(0, 200) || "Untitled",
          summary: item.contentSnippet?.slice(0, 500) || item.content?.slice(0, 500) || "",
          content: item["content:encoded"] || item.content || undefined,
          url: item.link || "#",
          source: source.name,
          sourceUrl: item.link || undefined,
          image: extractImage(item) || undefined,
          category: source.category,
          tags: extractTags(item.title || "", item.categories || []),
          publishedAt: item.pubDate ? new Date(item.pubDate).toISOString() : new Date().toISOString(),
          isHot: false,
          isFeatured: false,
          viewCount: 0,
        };
        
        // è¿™é‡Œå¯ä»¥æ·»åŠ åˆ°æ•°æ®åº“
        console.log(`[RSS] Added: ${newsItem.title}`);
        totalItems++;
      }
      
      console.log(`[RSS] ${source.name}: ${feed.items.length} items`);
    } catch (e) {
      const errorMsg = `Error crawling ${source.name}: ${e instanceof Error ? e.message : String(e)}`;
      console.error(errorMsg);
      errors.push(errorMsg);
    }
  }

  console.log(`[RSS] Total crawled: ${totalItems} items`);
  return { success: errors.length === 0, count: totalItems, errors };
}

// æŠ“å– GitHub Trending
export async function crawlGitHub(): Promise<{ success: boolean; count: number; error?: string }> {
  const GITHUB_TOKEN = process.env.GITHUB_TOKEN;
  
  if (!GITHUB_TOKEN) {
    console.warn("âš ï¸ GITHUB_TOKEN not set, skipping GitHub crawl");
    return { success: false, count: 0, error: "GITHUB_TOKEN not configured" };
  }

  let totalItems = 0;

  try {
    // æœç´¢ AI ç›¸å…³çš„çƒ­é—¨ä»“åº“
    const queries = [
      { q: "artificial intelligence stars:>5000", category: "AI" },
      { q: "machine learning stars:>5000", category: "æœºå™¨å­¦ä¹ " },
      { q: "LLM stars:>3000", category: "å¤§æ¨¡å‹" },
      { q: "AI agent stars:>1000", category: "AI Agent" },
    ];

    for (const { q, category } of queries) {
      try {
        const response = await fetch(
          `https://api.github.com/search/repositories?q=${encodeURIComponent(q)}&sort=stars&order=desc&per_page=5`,
          {
            headers: {
              Authorization: `Bearer ${GITHUB_TOKEN}`,
              Accept: "application/vnd.github.v3+json",
              "User-Agent": "NewClaw-Pro",
            },
          }
        );

        if (!response.ok) {
          console.error(`GitHub API error: ${response.status}`);
          continue;
        }

        const data = await response.json();
        
        for (const repo of data.items || []) {
          console.log(`[GitHub] ${category}: ${repo.name} (${repo.stargazers_count} stars)`);
          totalItems++;
        }
      } catch (e) {
        console.error(`Error querying GitHub:`, e);
      }
    }

    console.log(`[GitHub] Total crawled: ${totalItems} repos`);
    return { success: true, count: totalItems };
  } catch (error) {
    console.error("[GitHub] Error:", error);
    return { success: false, count: totalItems, error: error instanceof Error ? error.message : "Unknown error" };
  }
}

// æŠ“å– Hacker News AI ç›¸å…³å†…å®¹
export async function crawlHackerNews(): Promise<{ success: boolean; count: number; error?: string }> {
  try {
    // è·å–çƒ­é—¨æ•…äº‹
    const response = await fetch("https://hacker-news.firebaseio.com/v0/topstories.json");
    const storyIds = await response.json() as number[];
    
    let aiStories = 0;
    const maxCheck = Math.min(50, storyIds.length);
    
    for (let i = 0; i < maxCheck; i++) {
      const storyResponse = await fetch(`https://hacker-news.firebaseio.com/v0/item/${storyIds[i]}.json`);
      const story = await storyResponse.json();
      
      if (story && story.title) {
        const title = story.title.toLowerCase();
        if (title.includes('ai') || title.includes('llm') || title.includes('gpt') || 
            title.includes('machine learning') || title.includes('openai') || 
            title.includes('claude') || title.includes('gemini')) {
          console.log(`[HN] AI Story: ${story.title}`);
          aiStories++;
        }
      }
    }
    
    console.log(`[HN] Found ${aiStories} AI-related stories`);
    return { success: true, count: aiStories };
  } catch (error) {
    console.error("[HN] Error:", error);
    return { success: false, count: 0, error: error instanceof Error ? error.message : "Unknown error" };
  }
}

// æŠ“å– Product Hunt (éœ€è¦ API Key)
export async function crawlProductHunt(): Promise<{ success: boolean; count: number; error?: string }> {
  const PH_TOKEN = process.env.PRODUCT_HUNT_TOKEN;
  
  if (!PH_TOKEN) {
    console.warn("âš ï¸ PRODUCT_HUNT_TOKEN not set, using mock data");
    return { success: false, count: 0, error: "PRODUCT_HUNT_TOKEN not configured" };
  }

  try {
    const response = await fetch("https://api.producthunt.com/v2/api/graphql", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${PH_TOKEN}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        query: `
          query {
            posts(first: 10) {
              edges {
                node {
                  id
                  name
                  tagline
                  url
                  votesCount
                  topics {
                    edges {
                      node {
                        name
                      }
                    }
                  }
                }
              }
            }
          }
        `
      }),
    });

    const data = await response.json();
    const posts = data.data?.posts?.edges || [];
    
    // è¿‡æ»¤ AI ç›¸å…³äº§å“
    const aiPosts = posts.filter((p: any) => {
      const topics = p.node.topics?.edges?.map((t: any) => t.node.name.toLowerCase()) || [];
      return topics.some((t: string) => t.includes('ai') || t.includes('artificial intelligence'));
    });
    
    console.log(`[PH] Found ${aiPosts.length} AI products`);
    return { success: true, count: aiPosts.length };
  } catch (error) {
    console.error("[PH] Error:", error);
    return { success: false, count: 0, error: error instanceof Error ? error.message : "Unknown error" };
  }
}

// è¾…åŠ©å‡½æ•°ï¼šæå–å›¾ç‰‡
function extractImage(item: any): string | null {
  // å°è¯•ä»å†…å®¹ä¸­æå–å›¾ç‰‡
  const content = item["content:encoded"] || item.content || "";
  const imgMatch = content.match(/<img[^>]+src="([^"]+)"/);
  if (imgMatch) return imgMatch[1];
  
  // å°è¯•ä» media å†…å®¹ä¸­æå–
  if (item.media?.content?.url) return item.media.content.url;
  if (item.enclosure?.url) return item.enclosure.url;
  
  return null;
}

// è¾…åŠ©å‡½æ•°ï¼šæå–æ ‡ç­¾
function extractTags(title: string, categories: string[]): string[] {
  const tags: string[] = [];
  const lowerTitle = title.toLowerCase();
  
  // å¸¸è§ AI æ ‡ç­¾
  const tagMap: Record<string, string[]> = {
    'gpt': ['GPT', 'OpenAI'],
    'claude': ['Claude', 'Anthropic'],
    'gemini': ['Gemini', 'Google'],
    'llama': ['Llama', 'Meta'],
    'deepseek': ['DeepSeek'],
    'openai': ['OpenAI'],
    'google': ['Google'],
    'anthropic': ['Anthropic'],
    'meta': ['Meta'],
    'xai': ['xAI'],
    'grok': ['Grok', 'xAI'],
    'ai agent': ['AI Agent'],
    'multimodal': ['å¤šæ¨¡æ€'],
    'reasoning': ['æ¨ç†æ¨¡å‹'],
  };
  
  for (const [keyword, tagList] of Object.entries(tagMap)) {
    if (lowerTitle.includes(keyword)) {
      tags.push(...tagList);
    }
  }
  
  // æ·»åŠ åˆ†ç±»æ ‡ç­¾
  if (categories) {
    tags.push(...categories.slice(0, 3));
  }
  
  return [...new Set(tags)];
}

// è¿è¡Œæ‰€æœ‰æŠ“å–ä»»åŠ¡
export async function crawlAll() {
  console.log("\nğŸš€ Starting crawl at", new Date().toISOString());
  console.log("=" .repeat(50));
  
  const results = await Promise.allSettled([
    crawlRSS(),
    crawlGitHub(),
    crawlHackerNews(),
    crawlProductHunt(),
  ]);

  const summary = {
    rss: results[0],
    github: results[1],
    hackernews: results[2],
    producthunt: results[3],
  };

  console.log("=" .repeat(50));
  console.log("âœ… Crawl completed at", new Date().toISOString());
  
  return summary;
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶
if (require.main === module) {
  crawlAll().then((results) => {
    console.log("\nğŸ“Š Crawl Summary:");
    console.log(JSON.stringify(results, null, 2));
    process.exit(0);
  }).catch((error) => {
    console.error("âŒ Crawl failed:", error);
    process.exit(1);
  });
}
